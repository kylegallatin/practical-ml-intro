{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning - 0 to 100\n",
    "\n",
    "Machine learning and AI can be broadly described as software which attempts to mimic human intelligence. This definition attempts to cover everything in the field - from the days of nested `if` statements to the complex, mathematical functions we see today. Below is the \"classic\" view of machine learning.\n",
    "\n",
    "![image1](../img/image1.jpg)\n",
    "\n",
    "We're going to look at the **machine learning** section of the diagram. Modern machine learning is mathematically motivated, with the goal being to teach a machine to learn some task. More commonly, this \"machine\" isn't a conscious, sentient being, but the parameters of a mathematical function. The _input_ to machine learning is data, and the _output_ is a function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this Tutorial\n",
    "\n",
    "1. Understand _fundamentally_ how machine learning works and the \"general\" math behind it.\n",
    "2. Be able to apply some practical skills to classic machine learning problems.\n",
    "3. Better identify potential machine learning use cases in software applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Jupyter Overview\n",
    "\n",
    "This is a Python jupyter notebook. You can press **shift+enter** to run through code/markdown cells. The Python environment will be modified as you go. \n",
    "\n",
    "These notebooks are popular for teaching and experimentation. The markdown cells provide nice compliments to segments of code, and the notebook format lends itself well to the iterative nature of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some python code\n",
    "x = 5\n",
    "y = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more code\n",
    "print(x+y) # add\n",
    "print(x-y) # subtract\n",
    "print(x*y) # multiply\n",
    "print(x**y) # exponent\n",
    "print(x/y) # division\n",
    "print(x//y) # floor division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Super Trivial Amount of Math - Linear Regression\n",
    "\n",
    "Linear regression is often considered the basis of machine learning. It involves fitting a line to some data $x$ in order to predict $y$, where $y$ is a continous variable. You remember this from 8th grade as $y = mx + b$.\n",
    "\n",
    "For instance say we have data from two variables that have a linear correlation, $x$ and $y$. We want to create a function with input $x$ that we can use to predict $y$, We want to find and represent the _relationship_ between these two variables.\n",
    "\n",
    "| x   | y   |\n",
    "|-----|-----|\n",
    "| 1.2 | 1.4 |\n",
    "| 2.1 | 2.5 |\n",
    "| 3.3 | 3.6 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# python lists to represent the data\n",
    "x = [[1.2],[2.1],[3.3]] \n",
    "y = [1.4,2.5,3.6]\n",
    "\n",
    "# plot the points in 2 dimensions\n",
    "plt.scatter(x, y) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we model this relationship using the classic linear function $y = mx + b$, this will be our _objective function_, the final output of our machine learning problem. \n",
    "\n",
    "The actual machine learning involves finding the *best values* for $m$ and $b$. Let's try some example values to see how they look for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess our unknown m and b values to find a good line\n",
    "m = 2\n",
    "b = 1\n",
    "\n",
    "# function for y=mx+b\n",
    "def function(x, m, b):\n",
    "    y = m*x + b\n",
    "    return y\n",
    "\n",
    "def get_predictions(x):\n",
    "    # empty list\n",
    "    predictions = []\n",
    "\n",
    "    # for each data point in x\n",
    "    for point in x:\n",
    "        # pass it through the y=mx+b function to get our \"y\" guess\n",
    "        predictions.append(function(point[0], m, b))\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "predictions = get_predictions(x)\n",
    "\n",
    "# plot line and points\n",
    "plt.scatter(x, y) \n",
    "plt.plot(x, predictions) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consistently \"guess\" these parameters, but to actually find the best function we need to approach this mathematically. What if we had a way to define how \"good\" a model was? We can actually do this by trying to minimize the prediction error.\n",
    "\n",
    "By choosing parameters that minimize the prediction error, we get the best, most \"intelligent\" function to describe our data. Now we need a function for error, or as it's known a _cost function_. \n",
    "\n",
    "A classic cost function here would be the mean squared error, the difference between the actual values ($y$) and the values predicted by the current parameters of our objective function.\n",
    "\n",
    "*Objective function (function we want to fit):* $y = mx + b$\n",
    "\n",
    "*Cost function (determines best params for objective):* $(\\frac{1}{n})\\sum_{i=1}^{n}(y_{i} - (mx_{i} + b))^{2}$\n",
    "\n",
    "Let's calculate our error in Python a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "# play with m and b\n",
    "m = 2\n",
    "b = 1\n",
    "\n",
    "# get predictions for our chosen m and b values\n",
    "predictions = get_predictions(x)\n",
    "\n",
    "# calculate the cost\n",
    "mse = np.square(np.subtract(y, predictions)).mean() \n",
    "\n",
    "print(f\"MSE for b={b} and m={m} is: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look closer at our cost function. Notice when we plot this function, we get a parabola. The bottom of that parabola represents the minimum error, and the best parameters of our objective function!\n",
    "\n",
    "To actually find that minimum, machine learning uses calculus and linear algerbra which we'll skip today. But conceptually,it's relly easy just to picture this:\n",
    "\n",
    "![image](../img/image4.jpg)\n",
    "\n",
    "Let's show how we might do actual linear regression using standard ML Python libraries (you can also do this easily from scratch, but it's a bit overbearing for this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common ml library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate class and train model on our data\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)\n",
    "\n",
    "# print b and m\n",
    "print(regressor.intercept_)\n",
    "print(regressor.coef_) # y = 1.04x + 0.21 is the function that minimizes the error for hits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(x)\n",
    "\n",
    "# show points\n",
    "plt.scatter(x, y)\n",
    "# plot line (our predictions)\n",
    "plt.plot(x, predictions) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we may want to consider here might be a metric to optimize. While mean squared error is our cost function, the \"error\" (difference between y and y predictions) can vary wildly. If we're predicting values in the range (0,1) then our error will be miniscule compared to predicting values in the millions.\n",
    "\n",
    "Common metrics are things like **accuracy**: the percentage of correctly classified inputs. These metrics are consistent and interpretable. \n",
    "\n",
    "For this problem, we are predicting a continuous output and thus need to adapt a bit. A common metric for problems like this is **$R^2$** **(R squared)**: the proportion of variance explained by the data. Not going to go deeply into this, but like accuracy it ranges between (0,1) and is a good measure of how well a model is performing.\n",
    "\n",
    "Most machine learning packages have a default `score` function to analyze the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data, basically perfect model with a 1.0 R^2\n",
    "regressor.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: to view docs on a class, method or function use the ?\n",
    "?regressor.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example just uses one \"feature\" (common term for function inputs), $x$, but we can have as many features as we like. If we expand our function to $y = m_1x_1 + m_2x_2 + b$, we can use more than one feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# now we have 2 features, x1 and x2, so we have 2 coefficients to approximate\n",
    "x = [[1.2, 0.7],[2.1, 1.2],[3.3, 1.6]] \n",
    "\n",
    "# separate the features for plotting purposes\n",
    "x1 = [i[0] for i in x]\n",
    "x2 = [i[1] for i in x]\n",
    "\n",
    "# same as before\n",
    "y = [1.4,2.5,3.6]\n",
    "\n",
    "# 3d plot now that we have more than 2 dimensions\n",
    "ax = fig.add_subplot(111, projection='3d') \n",
    "ax.scatter(x1, x2, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)\n",
    "\n",
    "print(regressor.intercept_)\n",
    "print(regressor.coef_) # y = 0.45x_1 + 1.38x_2 - 0.112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning is typically bucketed into two categories: supervised and unsupervised. Today we'll focus on supervised.\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning has a known output value $y$, whereas unsupervised there is no known value. The example we did above is supervised, since we have a known $y$ value with which to train our data.\n",
    "\n",
    "### Unsupervised Learning\n",
    "We won't focus on this today, but unsupervised learning is when you have input features $x$ but no $y$ examples in the data. You can performs actions such as clustering, which group the data based on their distributions - but you can't perform the \"prediction\" of any known values.\n",
    "\n",
    "### Regression\n",
    "Within supervised learning, there are also two general categories: regression and classification. The key difference is in the output type. In the above example we were doing regression, predicting a _continuous_ variable output rangine from negative infinity to positive infinity. An example would be predicting house prices using data on the homes (although house prices in practice obviously have limitations).\n",
    "\n",
    "**Some standard regression models:**\n",
    "- linear regression\n",
    "- polynomial regression (same but with polynomial function, $y = mx^2 + b$)\n",
    "- ridge and lasso regression (regression with a penalization for large coefficients)\n",
    "- decision tree/random forest regressor (recursive algorithm that generates a tree of splits)\n",
    "\n",
    "\n",
    "### Classification\n",
    "Classification instead focuses on a discrete set of values, such as a binary classifion (predicting 0 or 1 for a given input). An example of this would be predicting whether or not someone has a malignant tumor, based on the input of tumor size, weight, or any other features.\n",
    "\n",
    "**Some standard classification models:**\n",
    "- logistic regression (transformed linear regression to return probability for discrete classes, 0 or 1)\n",
    "- decision tree or random forest (recursive tree based models that resolve to a class)\n",
    "- support vector machine (model decision boundary between classes)\n",
    "- deep learning models (image classification, text classification)\n",
    "\n",
    "![image3](../img/image3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Regression Example - House Prices\n",
    "\n",
    "Let's do a regression problem with a real dataset. This will be a regression problem where we want to predict the price of a house based on the features of the house (bedrooms, bathrooms, etc...). For more info on the dataset you can refer to [kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction#). I have included the data here for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe library for tables, lots of sql-like functionality \n",
    "import pandas as pd \n",
    "\n",
    "# load datafram from csv file\n",
    "df = pd.read_csv(\"kc_house_data.csv\") \n",
    "\n",
    "# first 5 rows\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` library is a data science and machine learning staple. It comes with a ton of OOTB features for data analysis, manipulation and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at bedrooms\n",
    "df['bedrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "A basic strategy in data science to explore, understand, and engineer data in accordance with the problem. Blindly trining models is easy, but understanding the real world and statistical relationship between variables is the \"scientific\" aspect of data science. \n",
    "\n",
    "For instance, I might assume that the more square footage a house has - the more expensive it is going to be. Let's take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['sqft_above'], df['price'])\n",
    "plt.xlabel(\"sqft\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what while the data are not perfectly linear, in general the price (y-axis) does seem to increase with square footage (x-axis).\n",
    "\n",
    "In a real situation, we might do many more plot and charts - but for the purposes of this demo let's say this is as far as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In addition to the data our model came with, we might want to include new features using our own domain knowledge. Adding human knowledge to a model often gives them an edge, especially if the model itself is relatively simple and incapable of capturing the vast complexities of data.\n",
    "\n",
    "Let's create a new feature in our data using other features. We're going to sum the other sqft features into a `total_sqft` feature, something people might consider when pricing a house.\n",
    "\n",
    "Obviously, this is a trivial example. In the real word, there may be more complex transformations or even additional data sources we may have to clean and take into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_features = ['sqft_living','sqft_lot','sqft_above','sqft_basement']\n",
    "\n",
    "# sum the features above into one column\n",
    "df['total_sqft'] = df[sqft_features].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Part\n",
    "\n",
    "Contrary to popular belief, the actual machine learning portion of machine learning problems is pretty small. most data are messy, and you spend like...90% of the time trying to massage and engineer the data as opposed to training dope models that change the landscape of software forever. The actual machine learning part usually follows a workflow like this:\n",
    "\n",
    "1. Split the data into a \"train\" set for training and a \"test\" set for evaluation\n",
    "2. Choose a model and features we want to fit to the data\n",
    "3. Choose a metric to optimize (accuracy for classification, or R squared for regression)\n",
    "4. Choose the model specific hyperparameters (model \"settings\")\n",
    "5. Train the model on the train data\n",
    "6. Evalaute on the test set using the metric\n",
    "\n",
    "I totally stole some of the following code from [this medium article](https://towardsdatascience.com/regression-using-sklearn-on-kc-housing-dataset-1ac80ca3d6d4). We're going to use 2 models, a classic linear regression and a more performant tree based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features we want to use\n",
    "features = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_above\",\"grade\",\n",
    "            \"floors\",\"view\",'sqft_lot','floors','waterfront','zipcode','total_sqft'] # choose features\n",
    "\n",
    "# getting those features from the dataframe\n",
    "x = df[features]\n",
    "y = df[\"price\"]\n",
    "\n",
    "# splits data into 80% train 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, \n",
    "                                                    random_state=3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train two regression models, a classic multiple linear regression and a more complex tree based model. We will compared the $R^2$ score (classic regression metric that ranges between 0 and 1) for our train and test datasets. \n",
    "\n",
    "The goal is to have an $R^2$ as close to 1.0 as possible for both the train AND test datasets. Having a high training dataset score but a lower test dataset score means that our model is _overfitting_ to the data. It's common to have models perform well on data they've seen before, but poorly on unforeseen data. This means the model generlizes poorly to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() # choose model\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Train R Squared: \", model.score(x_train, y_train))\n",
    "print(\"Test R Squared: \", model.score(x_test, y_test)) # low train and test r squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100) # choose model with settings\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Train R Squared: \", model.score(x_train, y_train))\n",
    "print(\"Test R Squared: \", model.score(x_test, y_test)) # very high train score, but model is overfit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Machine Learning Problems and Real World Examples\n",
    "\n",
    "The use cases for machine learning vary wildly from org to org. Some groups use it primarily for business intelligence and expoloration, while groups in R&D may use it to optimize drug design and discovery.\n",
    "\n",
    "As a person who primarily works with unstructured data (text, images) I have **2 key requirements** a machine learning project has to meet:\n",
    "\n",
    "1. There is some tedious or manual process that currently requires _human_ decision making\n",
    "2. There is data (preferably already labeled) for the problem at hand \n",
    "\n",
    "Imagine a pharmaceutical company wants to use text examples to automate the process of finding claims about their drugs from free text. They would need 2 things:\n",
    "\n",
    "1. **positive examples** that exemplify the target class: drug claims \n",
    "2. **negative examples** that do not exemplify this: literally any other kind of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "claims = [\"This drug is good\",\n",
    "         \"Drugs are bad\",\n",
    "         \"Actually drugs are tight\",\n",
    "         \"Big fan of drugs they're great\",\n",
    "         \"Drugs helped me with xyz problem\"]\n",
    "\n",
    "not_claims = [\"beep beep boop\",\n",
    "             \"jeffrey epstein didn't kill himself\",\n",
    "             \"and then just\",\n",
    "             \"some other random examples\",\n",
    "             \"of miscellanous text\"]\n",
    "\n",
    "# this is where I start to get weird and lazy with the code\n",
    "tmp = list(map(lambda x: (x, 1), claims)) + list(map(lambda x: (x, 0), claims))\n",
    "df = pd.DataFrame(tmp, columns = ['text','is_claim'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have those things, it's basically in the bag. Machine learning is fueled, and entirely dependent on sound data. Regardless of how cool the math or model is. The primary philosophy of ML is \"garbage in, garbage out\". The better the curated dataset, the more possibilities.\n",
    "\n",
    "Just for fun, here's some code to build a model to classify the above text. Since machine learning a mathematical process, we either need to generate numerical features for the words using their attributes (is_drug, POS_tag, etc...), or we can just vectorize them. \n",
    "\n",
    "Here I use tf-idf (term frequency inverse document frequency) and then build a quick classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorize text\n",
    "tfidfconverter = TfidfVectorizer()\n",
    "X = tfidfconverter.fit_transform(df[\"text\"]).toarray()\n",
    "\n",
    "# so little data, the model is no better than random guess\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier.fit(X, df[\"is_claim\"])\n",
    "classifier.score(X, df[\"is_claim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning really isn't a code heavy process, it's almost all in the artifacts generated from experiments (model type, settings, data features, transformations, etc...). Hence the \"scientific\" part of data science. The nice thing is that many of the artifacts generated by other are open source, and can easily be reused (like code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Fun Stuff - Pretrained Models\n",
    "\n",
    "The internet is littered with models you can load and use in like...a line of code. The SOTA (state of the art) models are often just an import away, making it super easy to incorporate machine learning in your appliction out of the box for general use cases. Take these easy steps from the `transformers` library - one of the most popular libraries in natural language processing.\n",
    "\n",
    "Here are some examples from their [docs](https://huggingface.co/transformers/main_classes/pipelines.html). **Note**: these are massive models and will take up some compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(\"I love this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These also support text classification, question answering, chatbot conversations and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
